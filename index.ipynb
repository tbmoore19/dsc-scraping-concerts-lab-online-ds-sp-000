{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Concerts - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you've seen how to scrape a simple website, it's time to again practice those skills on a full-fledged site!\n",
    "In this lab, you'll practice your scraping skills on a music website: https://www.residentadvisor.net.\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Create a full scraping pipeline that involves traversing over many pages of a website, dealing with errors and storing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Website\n",
    "\n",
    "For this lab, you'll be scraping the https://www.residentadvisor.net website. Start by navigating to the events page [here](https://www.residentadvisor.net/events) in your browser.\n",
    "\n",
    "<img src=\"images/ra.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the https://www.residentadvisor.net/events page in your browser.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "html_page = requests.get('https://www.residentadvisor.net/events')\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the Inspect Element Feature\n",
    "\n",
    "Next, open the inspect element feature from your web browser in order to preview the underlying HTML associated with the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the inspect element feature in your browser\n",
    "# soup.prettify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a Function to Scrape all of the Events on the Given Page Events Page\n",
    "\n",
    "The function should return a Pandas DataFrame with columns for the Event_Name, Venue, Event_Date and Number_of_Attendees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_column = soup.find('div', class_=\"fl col4\")\n",
    "event_listings = events_column.find_all('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events_column.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve event names\n",
    "event_names = [e.find('a').text for e in events]\n",
    "len(event_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Venue\n",
    "venues = [e.find('span').text[3:] for e in events]\n",
    "len(venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Dates\n",
    "event_dates = [e.text[:10] for e in events_column.find_all('time')]\n",
    "len(event_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve number of attendees\n",
    "attendees = [int(e.find('span').text) for e in events_column.find_all('p', class_='attending')]\n",
    "len(attendees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_names = []\n",
    "venues = []\n",
    "event_dates = []\n",
    "attendees = []\n",
    "\n",
    "for event in event_listings:\n",
    "    event_names.append(event.find('h1').find('a').text)\n",
    "    venues.append(event.find('h1').find('span').text[3:])\n",
    "    event_dates.append(event.find('time').text[:10])\n",
    "    \n",
    "    # Check if event has been cancelled\n",
    "    if event.find('p', class_='attending') is not None:\n",
    "        attendees.append(int(event.find('p', class_='attending').find('span').text))\n",
    "    else:\n",
    "        attendees.append(0)\n",
    "\n",
    "print(len(event_names), len(venues), len(event_dates), len(attendees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_events(num_events, events_page_url, names=[], venues=[], dts=[], attendees=[]):\n",
    "    \n",
    "    # Load the events page\n",
    "    html_page = requests.get(events_page_url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    \n",
    "    # Retrieve the event listings\n",
    "    events_column = soup.find('div', class_=\"fl col4\")\n",
    "    event_listings = events_column.find_all('article')\n",
    "    \n",
    "    # Loop through each event and retrieve name, venue, date and number of attendees\n",
    "    for event in event_listings:\n",
    "        if len(names) < num_events:\n",
    "            names.append(event.find('h1').find('a').text)\n",
    "            venues.append(event.find('h1').find('span').text[3:])\n",
    "            dts.append(event.find('time').text[:10])\n",
    "\n",
    "            # Check if event has been cancelled\n",
    "            if event.find('p', class_='attending') is not None:\n",
    "                attendees.append(int(event.find('p', class_='attending').find('span').text))\n",
    "            else:\n",
    "                attendees.append(0)\n",
    "    \n",
    "    return names, venues, dts, attendees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = []\n",
    "# venues = []\n",
    "# dts = []\n",
    "# attendees = []\n",
    "\n",
    "# test = scrape_events(20, 'https://www.residentadvisor.net/events', names, venues, dts, attendees)\n",
    "# print(len(names))\n",
    "# display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a Function to Retrieve the URL for the Next Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page(url):\n",
    "    \n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    \n",
    "    # Retrieve the href for the next arrow & concatenate to main page url\n",
    "    url_ext = soup.find('li', class_='but arrow-right right').find('a').attrs['href']\n",
    "    next_page_url = 'https://www.residentadvisor.net/' + url_ext\n",
    "    \n",
    "    return next_page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Next 1000 Events for Your Area\n",
    "\n",
    "Display the data sorted by the number of attendees. If there is a tie for the number attending, sort by event date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "def scrape_x_events(num_events, url, names=[], venues=[], dts=[], attendees=[]):\n",
    "    \n",
    "    if len(names) < num_events:\n",
    "        scrape_events(num_events, url, names, venues, dts, attendees)\n",
    "        url = next_page(url)\n",
    "        return scrape_x_events(num_events, url, names, venues, dts, attendees)\n",
    "    else:\n",
    "        df = pd.DataFrame([names, venues, dts, attendees]).transpose()\n",
    "        df.columns = [\"Event_Name\", \"Venue\", \"Event_Date\", \"Number_of_Attendees\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.residentadvisor.net/events'\n",
    "# Have reduced to 200 events for now as minimal events in calender due to COVID-19\n",
    "num_events = 200\n",
    "names = []\n",
    "venues = []\n",
    "dts = []\n",
    "attendees = []\n",
    "num_pages = 0\n",
    "\n",
    "event_table = scrape_x_events(num_events, url, names, venues, dts, attendees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_table = event_table.sort_values(by=['Number_of_Attendees', 'Event_Date'], ascending=[False, True])\n",
    "event_table.reset_index(drop=True, inplace=True)\n",
    "event_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Congratulations! In this lab, you successfully developed a pipeline to scrape a website for concert event information!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
